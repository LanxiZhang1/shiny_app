) %>% ungroup()
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_WAP = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1)) %>%
ungroup()
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_WAP = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1++ ask_size2 + bid_size2)) %>%
ungroup()
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_WAP = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1+ ask_size2 + bid_size2)) %>%
ungroup()
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_WAP = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1+ ask_size2 + bid_size2),
mean_order = tot_size/n()) %>%
ungroup()
head(features)
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_price = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1+ ask_size2 + bid_size2),
mean_order = tot_size/n()) %>%
ungroup()
head(features)
features <- stock1 %>%
mutate( time_bucket = ceiling(seconds_in_bucket / 30))%>% group_by(time_id,time_bucket) %>%
summarise(
mean_price = mean(WAP),
mean_BAS = mean(BidAskSpread),
tot_size = sum(ask_size1+bid_size1+ ask_size2 + bid_size2),
mean_order = tot_size/n()) %>%
ungroup()
head(features)
vol[[i]]
vol[[1]]
vol[[4]]
log_r1 <- list()
time_IDs <- unique(stock1$time_id)
for (i in 1 : length(time_IDs)) {
current_data <- stock1 %>% filter(time_id == time_IDs[i])
sec <- current_data$seconds_in_bucket
price <- current_data$WAP
log_r <- log(price[-1] / price[1:(length(price) - 1)])
log_r1[[i]] <- data.frame(time = sec[-1], log_return = log_r)
time.no.change <- (1:600)[!(1:600 %in% log_r1[[i]]$time)]
if (length(time.no.change) > 0) {
new.df <- data.frame(time = time.no.change, log_return = 0)
log_r1[[i]] <- rbind(log_r1[[i]], new.df)
log_r1[[i]] <- log_r1[[i]][order(log_r1[[i]]$time), ]
}
}
log_r1[[1]]
vol <- list()
comp_vol <- function(x) {
return(sqrt(sum(x ^ 2)))
}
for (i in 1 : length(log_r1)) {
log_r1[[i]] <- log_r1[[i]] %>% mutate(time_bucket = ceiling(time / 30))
vol[[i]] <- aggregate(log_return ~ time_bucket, data = log_r1[[i]], FUN = comp_vol)
colnames(vol[[i]]) <- c('time_bucket', 'volatility')
vol[[i]]$time_id <- time_IDs[i]
}
vol[[1]]
vol[[4]]
vol[[6]]
vol_df <- bind_rows(vol)
vol_df
vol_df <- bind_rows(vol)
vol_df[2]
df_vol <- bind_rows(vol)
model_data <- stock1_features %>%
inner_join(vol_df, by = c("time_id", "time_bucket"))
model_data <- stock1_features %>%
inner_join(vol_df, by = c("time_id", "time_bucket"))
reg_model <- features %>%
inner_join(df_vol,
by = c("time_id", "time_bucket"))
head(model_data)
reg_model <- features %>%
inner_join(df_vol,
by = c("time_id", "time_bucket"))
head(reg_model)
## combine all rows for every time id
df_vol <- bind_rows(vol)
reg_model <- features %>%
inner_join(df_vol,
by = c("time_id", "time_bucket"))
head(reg_model)
## train the model
lm_fit <- lm(volatility ~ mean_WAP + total_size + mean_BAS, data = model_data)
## train the model
lm_fit <- lm(volatility ~ mean_WAP + total_size + mean_BAS, data = reg_model)
## train the model
lm_fit <- lm(volatility ~ mean_price + tot_size + mean_BAS, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + tot_size + mean_BAS + mean_order, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + mean_BAS + mean_order, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + mean_BAS + mean_order, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + mean_BAS + mean_order, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + mean_BAS + mean_order, data = reg_model)
summary(lm_fit)
## train the model
lm_fit <- lm(volatility ~ mean_price + mean_BAS + tot_size + mean_order, data = reg_model)
summary(lm_fit)
size(reg_model)
len(reg_model)
summary(reg_model)
dim(reg_model)
predicted_vol <- predict(lm_fit, newdata = reg_model)
# Actual realized volatility from the validation set
actual_vol <- reg_model$volatility
# Calculate Mean Squared Error (MSE)
mse <- mean((actual_vol - predicted_vol)^2)
print(paste("MSE:", mse))
# Calculate QLIKE Loss
# One common formulation of QLIKE is:
qlike <- mean(log(predicted_vol) + actual_vol / predicted_vol)
print(paste("QLIKE Loss:", qlike))
View(reg_model)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(data.table)
# library(rugarch)
f <- "stock_1.csv"
readFile <- function(f){
df <-  fread(f)
}
stock = lapply(f, readFile)
stock = stock[[1]]
head(stock)
stock1 <- stock %>% mutate(
WAP = (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1),
BidAskSpread = ask_price1 / bid_price1 - 1)
head(stock1)
summary(stock1$WAP)
summary(stock1$BidAskSpread)
log_r1 <- list()
time_IDs <- unique(stock1$time_id)
for (i in 1 : length(time_IDs)) {
current_data <- stock1 %>% filter(time_id == time_IDs[i])
sec <- current_data$seconds_in_bucket
price <- current_data$WAP
log_r <- log(price[-1] / price[1:(length(price) - 1)])
log_r1[[i]] <- data.frame(time = sec[-1], log_return = log_r)
time.no.change <- (1:600)[!(1:600 %in% log_r1[[i]]$time)]
if (length(time.no.change) > 0) {
new.df <- data.frame(time = time.no.change, log_return = 0)
log_r1[[i]] <- rbind(log_r1[[i]], new.df)
log_r1[[i]] <- log_r1[[i]][order(log_r1[[i]]$time), ]
}
}
log_r1[[1]]
vol.train <- list()
vol.val <- list()
for (i in 1 : length(log_r1)) {
vol.train[[i]] <- vol[[i]][1:16, ]
vol.val[[i]] <- vol[[i]][-(1:16), ]
}
vol <- list()
comp_vol <- function(x) {
return(sqrt(sum(x ^ 2)))
}
for (i in 1 : length(log_r1)) {
log_r1[[i]] <- log_r1[[i]] %>% mutate(time_bucket = ceiling(time / 30))
vol[[i]] <- aggregate(log_return ~ time_bucket, data = log_r1[[i]], FUN = comp_vol)
colnames(vol[[i]]) <- c('time_bucket', 'volatility')
## combine with time id
vol[[i]]$time_id <- time_IDs[i]
}
vol[[1]]
ggplot(data = log_r1[[2000]], aes(x = time, y = log_return)) + geom_line()
ggplot(data = vol[[2000]], aes(x = time_bucket, y = volatility)) + geom_line() + geom_point()
vol.train <- list()
vol.val <- list()
for (i in 1 : length(log_r1)) {
vol.train[[i]] <- vol[[i]][1:16, ]
vol.val[[i]] <- vol[[i]][-(1:16), ]
}
list.reg <- list() # list for regression
stock1 <- stock1 %>% mutate(time_bucket = ceiling(seconds_in_bucket / 30),
num_order = bid_size1 + ask_size1 + bid_size2 + ask_size2)
len.train <- length(vol.train[[1]]$volatility)
for (i in 1 : length(vol)) {
stats.bucket <- stock1 %>%
filter(time_id == time_IDs[i] & time_bucket != 0) %>%
select(c(BidAskSpread, WAP, num_order, time_bucket))
# for each 30-sec time bucket, we compute the following statistics
mean.price <- aggregate(WAP ~ time_bucket, data = stats.bucket, FUN = mean)
mean.order <- aggregate(num_order ~ time_bucket, data = stats.bucket, FUN = mean)
mean.BAS <- aggregate(BidAskSpread ~ time_bucket, data = stats.bucket, FUN = mean)
list.reg[[i]] <- data.frame(volatility = vol.train[[i]]$volatility[-1],
price = mean.price$WAP[1:(len.train - 1)],
order = mean.order$num_order[1:(len.train - 1)],
BidAskSpread = mean.BAS$BidAskSpread[1:(len.train - 1)])
}
list.reg[[3]]
pooled_data <- bind_rows(list.reg)
head(pooled_data)
pooled_data <- bind_rows(list.reg)
dim(pooled_data)
head(pooled_data)
single_data <- bind_rows(list.reg)
single_data <- bind_rows(list.reg)
head(single_data)
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights = 0.8^(((len.tarin-2):0)/2))
single_data <- bind_rows(list.reg)
head(single_data)
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights = 0.8^(((len.tarin-2):0)/2))
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights = 0.8^(((len.train-2):0)/2))
weights1 <- 0.8^(((len.train - 2):0) / 2)
num_ids <- length(time_IDs)
weights_all <- rep(weights_one, times = num_ids)
weights1 <- 0.8^(((len.train - 2):0) / 2)
num_ids <- length(time_IDs)
weights_all <- rep(weights1, times = num_ids)
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights =  data = single_data,
weights1 <- 0.8^(((len.train - 2):0) / 2)
num_ids <- length(time_IDs)
weights_all <- rep(weights1, times = num_ids)
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights = weights_all)
summary(lm_model)
weights1 <- 0.8^(((len.train - 2):0) / 2)
num_ids <- length(time_IDs)
weights_all <- rep(weights1, times = num_ids)
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = single_data,
weights = weights_all)
summary(lm_model)
pred_val <- predict(lm_model,newdata = single_data)
MSE.lm <- mean((single_data$volatility - pred_val)^2)
MSE_lm
pred_val <- predict(lm_model,newdata = single_data)
MSE_lm <- mean((single_data$volatility - pred_val)^2)
MSE_lm
pred_val <- predict(lm_model,newdata = single_data)
# MSE
MSE_lm <- mean((single_data$volatility - pred_val)^2)
# RMSE
RMSE_lm <- sqrt(MSE_lm)
# QLIKE
observed_sq <- single_data$volatility^2
predicted_sq <- pred_val^2
QLIKE_lm <- mean(log(predicted_sq) + observed_sq / predicted_sq)
boxplot(MSE_lm,horizontal = TRUE)
pred_val <- predict(lm_model,newdata = single_data)
# MSE
MSE_lm <- mean((single_data$volatility - pred_val)^2)
# RMSE
RMSE_lm <- sqrt(MSE_lm)
# QLIKE
observed_sq <- single_data$volatility^2
predicted_sq <- pred_val^2
QLIKE_lm <- mean(log(predicted_sq) + observed_sq / predicted_sq)
boxplot(Qlike_lm,horizontal = TRUE)
boxplot(QLIKE_lm,horizontal = TRUE)
library(caret)
# Set up cross-validation
set.seed(123)
folds <- createFolds(pooled_data$volatility, k = 5, list = TRUE)
mse_values <- c()
qlike_values <- c()
for (fold in folds) {
train_data <- pooled_data[-fold, ]
test_data <- pooled_data[fold, ]
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = train_data)
pred <- predict(lm_model, newdata = test_data)
mse <- mean((test_data$volatility - pred)^2)
mse_values <- c(mse_values, mse)
# Calculate QLIKE for the fold
observed_sq <- test_data$volatility^2
predicted_sq <- pred^2
qlike <- mean(log(predicted_sq) + observed_sq / predicted_sq)
qlike_values <- c(qlike_values, qlike)
}
# Now create boxplots
boxplot(mse_values, horizontal = TRUE, main = "MSE Distribution")
boxplot(qlike_values, horizontal = TRUE, main = "QLIKE Distribution")
library(caret)
set.seed(123)
folds <- createFolds(pooled_data$volatility, k = 5, list = TRUE)
mse_values <- numeric()
qlike_values <- numeric()
for (fold in folds) {
train_data <- pooled_data[-fold, ]
test_data <- pooled_data[fold, ]
lm_model <- lm(volatility ~ price + order + BidAskSpread, data = train_data)
pred <- predict(lm_model, newdata = test_data)
mse <- mean((test_data$volatility - pred)^2)
mse_values <- c(mse_values, mse)
observed_sq <- test_data$volatility^2
predicted_sq <- pred^2
# Avoid potential issues with log(0) by ensuring predicted_sq > 0
if(any(predicted_sq <= 0)) {
warning("Some predicted squared volatilities are non-positive!")
}
qlike <- mean(log(predicted_sq) + observed_sq / predicted_sq)
qlike_values <- c(qlike_values, qlike)
}
print(qlike_values)
print(mse_values)
boxplot(mse_values, horizontal = TRUE, main = "MSE Distribution")
boxplot(qlike_values, horizontal = TRUE, main = "QLIKE Distribution")
pred_val <- predict(lm_model,newdata = single_data)
# MSE
MSE_lm <- mean((single_data$volatility - pred_val)^2)
# RMSE
RMSE_lm <- sqrt(MSE_lm)
# QLIKE
observed_sq <- single_data$volatility^2
predicted_sq <- pred_val^2
QLIKE_lm <- mean(log(predicted_sq) + observed_sq / predicted_sq)
MSE_lm
RMSE_lm
RMSE_lm
install.packages("xgboost")
str(vol.tarin)
str(vol.train)
dim(vol.train$data)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(data.table)
library(xgboost)
# library(rugarch)
features <- as.matrix(single_data[, c("price", "order", "BidAskSpread")])
target <- single_data$volatility
features <- as.matrix(single_data[, c("price", "order", "BidAskSpread")])
target <- single_data$volatility
set.seed(123)  # for reproducibility
train_index <- createDataPartition(target, p = 0.8, list = FALSE)
train_data <- features[train_index, ]
train_label <- target[train_index]
test_data <- features[-train_index, ]
test_label <- target[-train_index]
dtrain <- xgb.DMatrix(data = train_data, label = train_label)
dtest <- xgb.DMatrix(data = test_data, label = test_label)
params <- list(
objective = "reg:squarederror",  # Regression with squared error loss
eval_metric = "rmse",             # Evaluation metric RMSE
eta = 0.1,                        # Learning rate
max_depth = 3                     # Maximum tree depth (try tuning this)
)
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,                   # Number of boosting rounds
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10,
early_stopping_rounds = 10        # Stop if evaluation metric doesn't improve
)
summary(xgb_model)
MSE_xgb <- mean((test_label - pred_xgb)^2)
pred_xgb <- predict(xgb_model, newdata = dtest)
MSE_xgb <- mean((test_label - pred_xgb)^2)
RMSE_xgb <- sqrt(MSE_xgb)
RMSE_xgb
pred_xgb <- predict(xgb_model, newdata = dtest)
MSE_xgb <- mean((test_label - pred_xgb)^2)
RMSE_xgb <- sqrt(MSE_xgb)
SS_res <- sum((test_label - pred_xgb)^2)
SS_tot <- sum((test_label - mean(test_label))^2)
R2_xgb <- 1 - SS_res/SS_tot
R2_xgb
R2_xgb
shiny::runApp('GitHub/Optiver06/shiny_app')
runApp('GitHub/Optiver06/shiny/test.R')
runApp('GitHub/Optiver06/shiny_app')
runApp('GitHub/Optiver06/shiny_app')
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp('~/GitHub/Optiver06/shiny')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('~/GitHub/Optiver06/shiny')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
install.packages('rsconnect')
rsconnect::setAccountInfo(name='lzha', token='AFBDC14BC7D3483066DF2C970E7086EC', secret='m6UCno8/82PV8ELpxbzGCTjE4bMSiWPjdigwhF9n')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
library(rsconnect)
rsconnect::deployApp('C:\Users\Francine\Documents\GitHub\Optiver06\shiny_app\app.R')
runApp()
library(rsconnect)
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
install.packages("RcppArmadillo")
version
writeLines(Sys.which("make"))
Sys.getenv("PATH")
install.packages("RcppArmadillo")
library(rsconnect)
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
shiny::runApp()
knitr::opts_chunk$set(echo = TRUE)
load("RData")
ls()
# Subset the largest/most used data objects:
arima_predictions_list <- lapply(arima_predictions_list, function(df) head(df, 100))
ewma_final_predictions_list <- lapply(ewma_final_predictions_list, function(df) head(df, 100))
garch_forecast_list <- lapply(garch_forecast_list, function(df) head(df, 100))
hav_predictions_list <- lapply(hav_predictions_list, function(df) head(df, 100))
stock_feat <- head(stock_feat, 100)  # stock_feat is a data.frame
# (If your app uses stock_feat_filtered, subset it too)
stock_feat_filtered <- head(stock_feat_filtered, 100)
# (Optional: subset other data frames/lists if needed)
# Save only the objects used in your app:
save(
arima_predictions_list,
ewma_final_predictions_list,
garch_forecast_list,
hav_predictions_list,
stock_feat,
stock_feat_filtered,
file = "RData_small.RData"
)
runApp()
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
runApp()
runApp()
shiny::runApp()
library(rsconnect)
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/2shiny_app')
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
rsconnect::deployApp('C:/Users/Francine/Documents/GitHub/Optiver06/shiny_app')
shiny::runApp()
runApp()
runApp()
